{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Women's NCAA Tournament Model Predictions\n",
    "This notebook generates predictions for tournament matchups from 2021-2025 using Early and Elite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training datasets\n",
    "games_early = pd.read_csv('games_early.csv', index_col=0)\n",
    "games_elite = pd.read_csv('games_elite.csv', index_col=0)\n",
    "\n",
    "print(\"Early Games Shape:\", games_early.shape)\n",
    "print(\"Elite Games Shape:\", games_elite.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load Validation Matchups and Team Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation matchups\n",
    "validation_matchups = pd.read_csv('womens_matchups_validation_1_.csv')\n",
    "\n",
    "# Load team data for merging\n",
    "teams = pd.read_csv('wncaat_teams_historical_1_.csv')\n",
    "torvik = pd.read_csv('torvik_women_historical_1_.csv')\n",
    "\n",
    "print(\"Validation Matchups Shape:\", validation_matchups.shape)\n",
    "print(\"\\nFirst few validation matchups:\")\n",
    "print(validation_matchups.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Prepare Team Stats Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge teams with torvik data\n",
    "df = pd.merge(teams, torvik, on='torvik_id', how='inner')\n",
    "print(\"Merged Team Data Shape:\", df.shape)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[[\n",
    " 'team_id',\n",
    " 'wab',\n",
    " 'barthag',\n",
    " 'adj_oe',\n",
    " 'adj_de',\n",
    " 'efg_pct',\n",
    " 'efgd_pct',\n",
    " 'tor',\n",
    " 'tord',\n",
    " 'orb_pct',\n",
    " 'drb_pct',\n",
    " 'ftr',\n",
    " 'ftrd',\n",
    " '2p_pct',\n",
    " '2pd_pct',\n",
    " '3p_pct',\n",
    " '3pd_pct',\n",
    " '3pr',\n",
    " '3prd',\n",
    " 'adj_tempo'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform defensive stats (flip them so higher is better)\n",
    "df['adj_de']   = 200 - df['adj_de']\n",
    "df['efgd_pct'] = 100 - df['efgd_pct']\n",
    "df['tord']     = 100 - df['tord']\n",
    "df['drb_pct']  = 100 - df['drb_pct']\n",
    "df['ftrd']     = 100 - df['ftrd']\n",
    "df['2pd_pct']  = 100 - df['2pd_pct']\n",
    "df['3pd_pct']  = 100 - df['3pd_pct'] \n",
    "df['3prd']     = 100 - df['3prd']\n",
    "\n",
    "print(\"Team stats prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Restructure Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure the validation data from Team A/Team B format to high_team_id/low_team_id format\n",
    "validation_restructured = pd.DataFrame()\n",
    "validation_restructured['year'] = validation_matchups['Year']\n",
    "validation_restructured['region'] = validation_matchups['Region']\n",
    "validation_restructured['round'] = validation_matchups['Round']\n",
    "\n",
    "# Determine which team has the higher seed (lower seed number)\n",
    "validation_restructured['high_bracket_seed'] = validation_matchups[['Seed A', 'Seed B']].min(axis=1)\n",
    "validation_restructured['low_bracket_seed'] = validation_matchups[['Seed A', 'Seed B']].max(axis=1)\n",
    "\n",
    "# Assign teams based on seeds\n",
    "validation_restructured['high_team_id'] = validation_matchups.apply(\n",
    "    lambda row: row['Team A'] if row['Seed A'] <= row['Seed B'] else row['Team B'], axis=1\n",
    ")\n",
    "validation_restructured['low_team_id'] = validation_matchups.apply(\n",
    "    lambda row: row['Team B'] if row['Seed A'] <= row['Seed B'] else row['Team A'], axis=1\n",
    ")\n",
    "\n",
    "print(\"Validation data restructured:\")\n",
    "print(validation_restructured.head(10))\n",
    "print(f\"\\nShape: {validation_restructured.shape}\")\n",
    "print(f\"\\nYears: {sorted(validation_restructured['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Merge Validation Data with Team Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with team stats for high seed\n",
    "matchups = pd.merge(validation_restructured, df, left_on='high_team_id', right_on='team_id', how='left')\n",
    "matchups = matchups.rename(columns={\n",
    " 'wab': 'high_wab',\n",
    " 'barthag': 'high_barthag',\n",
    " 'adj_oe': 'high_adj_oe',\n",
    " 'adj_de': 'high_adj_de',\n",
    " 'efg_pct': 'high_efg_pct',\n",
    " 'efgd_pct': 'high_efgd_pct',\n",
    " 'tor': 'high_tor',\n",
    " 'tord': 'high_tord',\n",
    " 'orb_pct': 'high_orb_pct',\n",
    " 'drb_pct': 'high_drb_pct',\n",
    " 'ftr': 'high_ftr',\n",
    " 'ftrd': 'high_ftrd',\n",
    " '2p_pct': 'high_2p_pct',\n",
    " '2pd_pct': 'high_2pd_pct',\n",
    " '3p_pct': 'high_3p_pct',\n",
    " '3pd_pct': 'high_3pd_pct',\n",
    " '3pr': 'high_3pr',\n",
    " '3prd': 'high_3prd',\n",
    " 'adj_tempo': 'high_adj_tempo'\n",
    "})\n",
    "matchups = matchups.drop('team_id', axis=1)\n",
    "\n",
    "# Merge with team stats for low seed\n",
    "matchups = pd.merge(matchups, df, left_on='low_team_id', right_on='team_id', how='left')\n",
    "matchups = matchups.rename(columns={\n",
    " 'wab': 'low_wab',\n",
    " 'barthag': 'low_barthag',\n",
    " 'adj_oe': 'low_adj_oe',\n",
    " 'adj_de': 'low_adj_de',\n",
    " 'efg_pct': 'low_efg_pct',\n",
    " 'efgd_pct': 'low_efgd_pct',\n",
    " 'tor': 'low_tor',\n",
    " 'tord': 'low_tord',\n",
    " 'orb_pct': 'low_orb_pct',\n",
    " 'drb_pct': 'low_drb_pct',\n",
    " 'ftr': 'low_ftr',\n",
    " 'ftrd': 'low_ftrd',\n",
    " '2p_pct': 'low_2p_pct',\n",
    " '2pd_pct': 'low_2pd_pct',\n",
    " '3p_pct': 'low_3p_pct',\n",
    " '3pd_pct': 'low_3pd_pct',\n",
    " '3pr': 'low_3pr',\n",
    " '3prd': 'low_3prd',\n",
    " 'adj_tempo': 'low_adj_tempo'\n",
    "})\n",
    "matchups = matchups.drop('team_id', axis=1)\n",
    "\n",
    "print(\"Validation matchups merged. Shape:\", matchups.shape)\n",
    "print(f\"\\nMatches with missing data: {matchups.isnull().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing data (teams not in Torvik dataset)\n",
    "matchups_clean = matchups.dropna()\n",
    "print(f\"Clean matchups: {len(matchups_clean)} (removed {len(matchups) - len(matchups_clean)} with missing data)\")\n",
    "\n",
    "if len(matchups) > len(matchups_clean):\n",
    "    print(\"\\nTeams with missing stats:\")\n",
    "    missing = matchups[matchups.isnull().any(axis=1)]\n",
    "    print(\"High seeds:\", missing['high_team_id'].unique())\n",
    "    print(\"Low seeds:\", missing['low_team_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Create Differential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create differential features (following Games_Data_Prep.ipynb)\n",
    "matchups_final = pd.DataFrame()\n",
    "matchups_final['year']              = matchups_clean['year']\n",
    "matchups_final['region']            = matchups_clean['region']\n",
    "matchups_final['round']             = matchups_clean['round']\n",
    "matchups_final['high_bracket_seed'] = matchups_clean['high_bracket_seed']\n",
    "matchups_final['high_team_id']      = matchups_clean['high_team_id']\n",
    "matchups_final['low_bracket_seed']  = matchups_clean['low_bracket_seed']\n",
    "matchups_final['low_team_id']       = matchups_clean['low_team_id']\n",
    "matchups_final['wab']       = matchups_clean['high_wab']      - matchups_clean['low_wab']\n",
    "matchups_final['barthag']   = matchups_clean['high_barthag']  - matchups_clean['low_barthag']\n",
    "matchups_final['adj_oe']    = matchups_clean['high_adj_oe']   - matchups_clean['low_adj_de']\n",
    "matchups_final['adj_de']    = matchups_clean['high_adj_de']   - matchups_clean['low_adj_oe']\n",
    "matchups_final['efg_pct']   = matchups_clean['high_efg_pct']  - matchups_clean['low_efgd_pct']\n",
    "matchups_final['efgd_pct']  = matchups_clean['high_efgd_pct'] - matchups_clean['low_efg_pct']\n",
    "matchups_final['tor']       = matchups_clean['high_tor']      - matchups_clean['low_tord']\n",
    "matchups_final['tord']      = matchups_clean['high_tord']     - matchups_clean['low_tor']\n",
    "matchups_final['orb_pct']   = matchups_clean['high_orb_pct']  - matchups_clean['low_drb_pct']\n",
    "matchups_final['drb_pct']   = matchups_clean['high_drb_pct']  - matchups_clean['low_orb_pct']\n",
    "matchups_final['ftr']       = matchups_clean['high_ftr']      - matchups_clean['low_ftrd']\n",
    "matchups_final['ftrd']      = matchups_clean['high_ftrd']     - matchups_clean['low_ftr']\n",
    "matchups_final['2p_pct']    = matchups_clean['high_2p_pct']   - matchups_clean['low_2pd_pct']\n",
    "matchups_final['2pd_pct']   = matchups_clean['high_2pd_pct']  - matchups_clean['low_2p_pct']\n",
    "matchups_final['3p_pct']    = matchups_clean['high_3p_pct']   - matchups_clean['low_3pd_pct']\n",
    "matchups_final['3pd_pct']   = matchups_clean['high_3pd_pct']  - matchups_clean['low_3p_pct']\n",
    "matchups_final['3pr']       = matchups_clean['high_3pr']      - matchups_clean['low_3prd']\n",
    "matchups_final['3prd']      = matchups_clean['high_3prd']     - matchups_clean['low_3pr']\n",
    "matchups_final['adj_tempo'] = matchups_clean['high_adj_tempo']- matchups_clean['low_adj_tempo']\n",
    "\n",
    "print(\"Differential features created\")\n",
    "print(f\"Final dataset shape: {matchups_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Map Round Names and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map round names to match training data format\n",
    "round_mapping = {\n",
    "    'Round 1': 'First Round',\n",
    "    'Round 2': 'Second Round',\n",
    "    'Round 3': 'Sweet 16',\n",
    "    'Round 4': 'Elite Eight',\n",
    "    'Round 5': 'Final Four',\n",
    "    'Round 6': 'Championship'\n",
    "}\n",
    "\n",
    "matchups_final['round'] = matchups_final['round'].map(round_mapping)\n",
    "\n",
    "# Split validation data\n",
    "validation_early = matchups_final[matchups_final['round'].isin(['First Round', 'Second Round'])].copy()\n",
    "validation_elite = matchups_final[matchups_final['round'].isin(['Sweet 16', 'Elite Eight', 'Final Four', 'Championship'])].copy()\n",
    "\n",
    "print(f\"Early Round Games: {len(validation_early)}\")\n",
    "print(f\"Elite Round Games: {len(validation_elite)}\")\n",
    "print(f\"\\nYears in dataset: {sorted(matchups_final['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Train Early Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early model features from WOMEN'S EARLY.txt\n",
    "early_features = ['barthag', 'adj_oe', 'adj_de', 'orb_pct', 'drb_pct', 'ftr', '2p_pct']\n",
    "\n",
    "# Prepare training data\n",
    "X_early_train = games_early[early_features]\n",
    "y_early_train = games_early['win']\n",
    "\n",
    "# Train Logistic Regression model\n",
    "early_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "early_model.fit(X_early_train, y_early_train)\n",
    "\n",
    "print(\"Early Model (Logistic Regression) trained\")\n",
    "print(f\"Training accuracy: {early_model.score(X_early_train, y_early_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Train Elite Model (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elite model features from WOMEN'S ELITE.txt\n",
    "elite_features = ['wab', 'barthag', 'adj_oe', 'adj_de', 'efg_pct', 'efgd_pct',\n",
    "                  'orb_pct', 'drb_pct', '2p_pct', '2pd_pct', '3p_pct', '3pd_pct', '3pr']\n",
    "\n",
    "# Prepare training data\n",
    "X_elite_train = games_elite[elite_features]\n",
    "y_elite_train = games_elite['win']\n",
    "\n",
    "# Train XGBoost model with parameters from WOMEN'S ELITE.txt\n",
    "elite_model = XGBClassifier(\n",
    "    learning_rate=0.2997738363859162,\n",
    "    max_depth=9,\n",
    "    min_child_weight=8.623522034407337,\n",
    "    subsample=0.8324211691115178,\n",
    "    colsample_bytree=0.9988769480719698,\n",
    "    gamma=2.017715776385069,\n",
    "    reg_alpha=0.9692563913308194,\n",
    "    reg_lambda=2.5910989850621258,\n",
    "    n_estimators=335,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "elite_model.fit(X_elite_train, y_elite_train)\n",
    "\n",
    "print(\"Elite Model (XGBoost) trained\")\n",
    "print(f\"Training accuracy: {elite_model.score(X_elite_train, y_elite_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b",
   "metadata": {},
   "source": [
    "## Apply Platt Scaling to Elite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost can produce poorly calibrated probabilities\n",
    "# Use Platt Scaling (logistic regression on the output) to calibrate\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Calibrate the elite model using cross-validation\n",
    "elite_model_calibrated = CalibratedClassifierCV(elite_model, cv=5, method='sigmoid')\n",
    "elite_model_calibrated.fit(X_elite_train, y_elite_train)\n",
    "\n",
    "print(\"Elite Model calibrated with Platt Scaling\")\n",
    "print(f\"Calibrated training accuracy: {elite_model_calibrated.score(X_elite_train, y_elite_train):.4f}\")\n",
    "\n",
    "# Compare uncalibrated vs calibrated probabilities on a sample\n",
    "sample_probs_uncalibrated = elite_model.predict_proba(X_elite_train[:10])[:, 1]\n",
    "sample_probs_calibrated = elite_model_calibrated.predict_proba(X_elite_train[:10])[:, 1]\n",
    "\n",
    "print(\"\\nSample probability comparison (first 10 training examples):\")\n",
    "print(\"Uncalibrated:\", [f\"{p:.3f}\" for p in sample_probs_uncalibrated])\n",
    "print(\"Calibrated:  \", [f\"{p:.3f}\" for p in sample_probs_calibrated])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Generate Early Round Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "X_early_val = validation_early[early_features]\n",
    "\n",
    "# Make predictions\n",
    "y_early_pred = early_model.predict(X_early_val)\n",
    "y_early_pred_proba = early_model.predict_proba(X_early_val)[:, 1]\n",
    "\n",
    "# Add predictions to validation dataframe\n",
    "validation_early['predicted_win'] = y_early_pred\n",
    "validation_early['win_probability'] = y_early_pred_proba\n",
    "\n",
    "print(f\"Early round predictions complete: {len(validation_early)} games\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(validation_early[['year', 'round', 'high_team_id', 'low_team_id', 'predicted_win', 'win_probability']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Generate Elite Round Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "X_elite_val = validation_elite[elite_features]\n",
    "\n",
    "# Make predictions using CALIBRATED model\n",
    "y_elite_pred = elite_model_calibrated.predict(X_elite_val)\n",
    "y_elite_pred_proba = elite_model_calibrated.predict_proba(X_elite_val)[:, 1]\n",
    "\n",
    "# Add predictions to validation dataframe\n",
    "validation_elite['predicted_win'] = y_elite_pred\n",
    "validation_elite['win_probability'] = y_elite_pred_proba\n",
    "\n",
    "print(f\"Elite round predictions complete: {len(validation_elite)} games\")\n",
    "print(\"\\nSample predictions:\")\n",
    "print(validation_elite[['year', 'round', 'high_team_id', 'low_team_id', 'predicted_win', 'win_probability']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Combine and Save All Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions\n",
    "all_predictions = pd.concat([validation_early, validation_elite], ignore_index=True)\n",
    "all_predictions = all_predictions.sort_values(['year', 'round', 'region'])\n",
    "\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "print(f\"Years covered: {sorted(all_predictions['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "all_predictions.to_csv('tournament_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'tournament_predictions.csv'\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of predictions:\")\n",
    "display_cols = ['year', 'round', 'high_team_id', 'high_bracket_seed', \n",
    "                'low_team_id', 'low_bracket_seed', 'predicted_win', 'win_probability']\n",
    "print(all_predictions[display_cols].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Create Readable Predictions Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more readable output showing which team is predicted to win\n",
    "predictions_readable = all_predictions.copy()\n",
    "predictions_readable['predicted_winner'] = predictions_readable.apply(\n",
    "    lambda row: row['high_team_id'] if row['predicted_win'] == 1 else row['low_team_id'], axis=1\n",
    ")\n",
    "predictions_readable['predicted_loser'] = predictions_readable.apply(\n",
    "    lambda row: row['low_team_id'] if row['predicted_win'] == 1 else row['high_team_id'], axis=1\n",
    ")\n",
    "\n",
    "# Save readable version\n",
    "predictions_readable.to_csv('tournament_predictions_readable.csv', index=False)\n",
    "print(\"Readable predictions saved to 'tournament_predictions_readable.csv'\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nReadable predictions sample:\")\n",
    "readable_cols = ['year', 'round', 'region', 'predicted_winner', 'predicted_loser', 'win_probability']\n",
    "print(predictions_readable[readable_cols].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal games predicted: {len(all_predictions)}\")\n",
    "print(f\"Years: {sorted(all_predictions['year'].unique())}\")\n",
    "\n",
    "print(\"\\nPredictions by round:\")\n",
    "for round_name in ['First Round', 'Second Round', 'Sweet 16', 'Elite Eight', 'Final Four', 'Championship']:\n",
    "    count = len(all_predictions[all_predictions['round'] == round_name])\n",
    "    if count > 0:\n",
    "        print(f\"  {round_name}: {count} games\")\n",
    "\n",
    "print(\"\\nPredictions by year:\")\n",
    "for year in sorted(all_predictions['year'].unique()):\n",
    "    count = len(all_predictions[all_predictions['year'] == year])\n",
    "    print(f\"  {year}: {count} games\")\n",
    "\n",
    "print(\"\\nHigher seed predicted to win:\")\n",
    "higher_seed_wins = (all_predictions['predicted_win'] == 1).sum()\n",
    "print(f\"  {higher_seed_wins} / {len(all_predictions)} ({100*higher_seed_wins/len(all_predictions):.1f}%)\")\n",
    "\n",
    "print(\"\\nWin probability statistics:\")\n",
    "print(f\"  Mean: {all_predictions['win_probability'].mean():.3f}\")\n",
    "print(f\"  Median: {all_predictions['win_probability'].median():.3f}\")\n",
    "print(f\"  Min: {all_predictions['win_probability'].min():.3f}\")\n",
    "print(f\"  Max: {all_predictions['win_probability'].max():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
