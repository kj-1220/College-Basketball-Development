{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uScljY9j-n4L",
        "outputId": "823b66aa-f701-4b4e-b667-9ef98899a2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "3-STAGE LightGBM WITH OVERFITTING PREVENTION\n",
            "================================================================================\n",
            "\n",
            "Total games: 320\n",
            "Class distribution:\n",
            "win\n",
            "1    160\n",
            "0    160\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"3-STAGE LightGBM WITH OVERFITTING PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/men_2026_matchups_training.csv')\n",
        "\n",
        "# Example: Second Round games (adjust as needed)\n",
        "df_filtered = df[df['round'] == 'Second Round'].copy()\n",
        "\n",
        "print(f\"\\nTotal games: {len(df_filtered)}\")\n",
        "print(f\"Class distribution:\\n{df_filtered['win'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (use your feature list)\n",
        "features = [\n",
        "    '5man_bpm', 'kenpom_rtg', 'def_lineup_depth_quality', 'torvik_rtg',\n",
        "    'kenpom_off', '3man_bpm', '5man_dbpm', 'lineup_depth_quality',\n",
        "    '5man_obpm', 'kenpom_def', 'experience_weighted_production', 'wab',\n",
        "    'defensive_versatility_score', 'four_factors_composite', 'torvik_def',\n",
        "    '3man_obpm', 'off_3pt_fg_pct', 'def_four_factors_composite', 'torvik_off',\n",
        "    'size_speed_index', 'def_size_speed_index', 'def_rim_efficiency',\n",
        "    'def_experience_impact', 'efg_pct', 'offensive_versatility_score',\n",
        "    'efgd_pct', '3pd_pct', 'def_3pt_fg_pct', 'blk_pct', 'blked_pct'\n",
        "]\n",
        "\n",
        "print(f\"\\nNumber of features: {len(features)}\")\n",
        "\n",
        "# Prepare X and y\n",
        "X = df_filtered[features]\n",
        "y = df_filtered['win']\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# Split: Train, Validation, Test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.18, random_state=42, stratify=y_temp  # ~15% of original\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {len(X_train)}\")\n",
        "print(f\"Validation set: {len(X_val)}\")\n",
        "print(f\"Test set: {len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9syR51Hd-t2c",
        "outputId": "9bea5520-851c-482c-fd33-68e9fc75f238"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of features: 30\n",
            "\n",
            "Training set: 223\n",
            "Validation set: 49\n",
            "Test set: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale features (optional for LightGBM but can help)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "a2XOF3Q3-xI0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CUSTOM LIGHTGBM WRAPPER WITH EARLY STOPPING\n",
        "# =============================================================================\n",
        "class LGBMWithEarlyStopping(lgb.LGBMClassifier):\n",
        "    \"\"\"LightGBM wrapper that uses early stopping during hyperparameter search\"\"\"\n",
        "\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        # Check if we have validation data from RandomizedSearchCV\n",
        "        if hasattr(self, '_val_data'):\n",
        "            X_val, y_val = self._val_data\n",
        "\n",
        "            return super().fit(\n",
        "                X, y,\n",
        "                eval_set=[(X, y), (X_val, y_val)],\n",
        "                eval_names=['train', 'valid'],\n",
        "                eval_metric='logloss',\n",
        "                callbacks=[\n",
        "                    lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "                    lgb.log_evaluation(period=0)  # Disable logging during search\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            return super().fit(X, y)\n",
        "\n",
        "    def set_validation_data(self, X_val, y_val):\n",
        "        \"\"\"Set validation data for early stopping\"\"\"\n",
        "        self._val_data = (X_val, y_val)"
      ],
      "metadata": {
        "id": "G1DWI3Cz-y5X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STAGE 1: BROAD SEARCH (500 iterations)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 1: BROAD HYPERPARAMETER SEARCH\")\n",
        "print(\"Iterations: 500 | CV Folds: 5 | Total Fits: 2,500\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Conservative parameter distributions to prevent overfitting\n",
        "param_distributions_stage1 = {\n",
        "    # Learning rate - LOWER prevents overfitting\n",
        "    'learning_rate': [0.01, 0.02, 0.03, 0.05, 0.07, 0.1],\n",
        "\n",
        "    # Tree complexity - LIMIT to prevent overfitting\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8, 10],\n",
        "    'num_leaves': [15, 20, 31, 40, 50, 63],\n",
        "    'min_child_samples': [10, 20, 30, 50, 70, 100],\n",
        "    'min_child_weight': [0.001, 0.01, 0.1, 1.0],\n",
        "\n",
        "    # Regularization - HIGHER prevents overfitting\n",
        "    'reg_alpha': [0, 0.01, 0.1, 0.5, 1.0, 5.0],\n",
        "    'reg_lambda': [0, 0.01, 0.1, 0.5, 1.0, 5.0],\n",
        "\n",
        "    # Sampling - LOWER prevents overfitting\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'subsample_freq': [0, 1],\n",
        "\n",
        "    # Other anti-overfitting parameters\n",
        "    'min_split_gain': [0.0, 0.01, 0.05, 0.1],\n",
        "    'max_bin': [127, 255, 511],\n",
        "\n",
        "    # Number of estimators (will use early stopping)\n",
        "    'n_estimators': [500, 1000, 1500]\n",
        "}\n",
        "\n",
        "lgbm_stage1 = LGBMWithEarlyStopping(\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Set validation data for early stopping\n",
        "lgbm_stage1.set_validation_data(X_val_scaled, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXcSlRPn-2I2",
        "outputId": "41af4085-e3dd-44f7-83b2-27ee15785cb5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 1: BROAD HYPERPARAMETER SEARCH\n",
            "Iterations: 500 | CV Folds: 5 | Total Fits: 2,500\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_search_stage1 = RandomizedSearchCV(\n",
        "    estimator=lgbm_stage1,\n",
        "    param_distributions=param_distributions_stage1,\n",
        "    n_iter=500,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "stage1_start = time.time()\n",
        "random_search_stage1.fit(X_train_scaled, y_train)\n",
        "stage1_time = time.time() - stage1_start\n",
        "\n",
        "best_params_stage1 = random_search_stage1.best_params_\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"STAGE 1 COMPLETE - Time: {stage1_time/60:.1f} minutes\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Best CV Score: {random_search_stage1.best_score_:.4f}\")\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in best_params_stage1.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Check for overfitting in Stage 1\n",
        "stage1_train_score = random_search_stage1.best_estimator_.score(X_train_scaled, y_train)\n",
        "stage1_val_score = random_search_stage1.best_estimator_.score(X_val_scaled, y_val)\n",
        "print(f\"\\nStage 1 Overfitting Check:\")\n",
        "print(f\"  Train accuracy: {stage1_train_score:.4f}\")\n",
        "print(f\"  Validation accuracy: {stage1_val_score:.4f}\")\n",
        "print(f\"  Gap: {stage1_train_score - stage1_val_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuM9Er2u--uH",
        "outputId": "8142168a-e72e-4d47-e25f-cd3d68ca6546"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
            "\n",
            "================================================================================\n",
            "STAGE 1 COMPLETE - Time: 16.3 minutes\n",
            "================================================================================\n",
            "Best CV Score: 0.7578\n",
            "\n",
            "Best Parameters:\n",
            "  subsample_freq: 1\n",
            "  subsample: 0.8\n",
            "  reg_lambda: 5.0\n",
            "  reg_alpha: 0.1\n",
            "  num_leaves: 20\n",
            "  n_estimators: 500\n",
            "  min_split_gain: 0.0\n",
            "  min_child_weight: 1.0\n",
            "  min_child_samples: 10\n",
            "  max_depth: 6\n",
            "  max_bin: 511\n",
            "  learning_rate: 0.02\n",
            "  colsample_bytree: 0.6\n",
            "\n",
            "Stage 1 Overfitting Check:\n",
            "  Train accuracy: 0.9865\n",
            "  Validation accuracy: 0.8163\n",
            "  Gap: 0.1702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMWithEarlyStopping was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMWithEarlyStopping was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STAGE 2: FOCUSED SEARCH (2,500 iterations)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 2: FOCUSED SEARCH AROUND BEST PARAMS\")\n",
        "print(\"Iterations: 2,500 | CV Folds: 5 | Total Fits: 12,500\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "def get_range(value, options_list, expand_by=2):\n",
        "    \"\"\"Get a focused range around the best value\"\"\"\n",
        "    if value is None:\n",
        "        return [None]\n",
        "    if value not in options_list:\n",
        "        return [value]\n",
        "\n",
        "    idx = options_list.index(value)\n",
        "    start_idx = max(0, idx - expand_by)\n",
        "    end_idx = min(len(options_list), idx + expand_by + 1)\n",
        "    return options_list[start_idx:end_idx]\n",
        "\n",
        "learning_rate_opts = [0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.07, 0.1, 0.15]\n",
        "max_depth_opts = [3, 4, 5, 6, 7, 8, 10, 12]\n",
        "num_leaves_opts = [10, 15, 20, 25, 31, 40, 50, 63, 80]\n",
        "min_child_samples_opts = [5, 10, 15, 20, 30, 50, 70, 100, 150]\n",
        "min_child_weight_opts = [0.0001, 0.001, 0.01, 0.1, 1.0, 5.0]\n",
        "reg_alpha_opts = [0, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "reg_lambda_opts = [0, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "subsample_opts = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
        "colsample_bytree_opts = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
        "min_split_gain_opts = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2]\n",
        "max_bin_opts = [127, 191, 255, 383, 511]\n",
        "n_estimators_opts = [500, 750, 1000, 1500, 2000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XWXx8pJ_BZ-",
        "outputId": "db913e85-21bb-4d1a-e24a-c91247c2dc85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 2: FOCUSED SEARCH AROUND BEST PARAMS\n",
            "Iterations: 2,500 | CV Folds: 5 | Total Fits: 12,500\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_distributions_stage2 = {\n",
        "    'learning_rate': get_range(best_params_stage1['learning_rate'], learning_rate_opts, 3),\n",
        "    'max_depth': get_range(best_params_stage1['max_depth'], max_depth_opts, 2),\n",
        "    'num_leaves': get_range(best_params_stage1['num_leaves'], num_leaves_opts, 3),\n",
        "    'min_child_samples': get_range(best_params_stage1['min_child_samples'], min_child_samples_opts, 3),\n",
        "    'min_child_weight': get_range(best_params_stage1['min_child_weight'], min_child_weight_opts, 2),\n",
        "    'reg_alpha': get_range(best_params_stage1['reg_alpha'], reg_alpha_opts, 3),\n",
        "    'reg_lambda': get_range(best_params_stage1['reg_lambda'], reg_lambda_opts, 3),\n",
        "    'subsample': get_range(best_params_stage1['subsample'], subsample_opts, 3),\n",
        "    'colsample_bytree': get_range(best_params_stage1['colsample_bytree'], colsample_bytree_opts, 3),\n",
        "    'subsample_freq': [best_params_stage1['subsample_freq']],\n",
        "    'min_split_gain': get_range(best_params_stage1['min_split_gain'], min_split_gain_opts, 2),\n",
        "    'max_bin': get_range(best_params_stage1['max_bin'], max_bin_opts, 1),\n",
        "    'n_estimators': get_range(best_params_stage1['n_estimators'], n_estimators_opts, 1)\n",
        "}\n",
        "\n",
        "print(f\"Stage 2 Search Space:\")\n",
        "for param, values in param_distributions_stage2.items():\n",
        "    print(f\"  {param}: {values}\")\n",
        "\n",
        "lgbm_stage2 = LGBMWithEarlyStopping(\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lgbm_stage2.set_validation_data(X_val_scaled, y_val)\n",
        "\n",
        "random_search_stage2 = RandomizedSearchCV(\n",
        "    estimator=lgbm_stage2,\n",
        "    param_distributions=param_distributions_stage2,\n",
        "    n_iter=2500,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=43,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "stage2_start = time.time()\n",
        "random_search_stage2.fit(X_train_scaled, y_train)\n",
        "stage2_time = time.time() - stage2_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFwgQrHu_Ene",
        "outputId": "bc6cc993-22d6-46d9-accc-226e38017f49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stage 2 Search Space:\n",
            "  learning_rate: [0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.07]\n",
            "  max_depth: [4, 5, 6, 7, 8]\n",
            "  num_leaves: [10, 15, 20, 25, 31, 40]\n",
            "  min_child_samples: [5, 10, 15, 20, 30]\n",
            "  min_child_weight: [0.01, 0.1, 1.0, 5.0]\n",
            "  reg_alpha: [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0]\n",
            "  reg_lambda: [0.5, 1.0, 2.0, 5.0, 10.0]\n",
            "  subsample: [0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
            "  colsample_bytree: [0.5, 0.6, 0.7, 0.75, 0.8]\n",
            "  subsample_freq: [1]\n",
            "  min_split_gain: [0.0, 0.001, 0.01]\n",
            "  max_bin: [383, 511]\n",
            "  n_estimators: [500, 750]\n",
            "Fitting 5 folds for each of 2500 candidates, totalling 12500 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_stage2 = random_search_stage2.best_params_\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"STAGE 2 COMPLETE - Time: {stage2_time/60:.1f} minutes\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Best CV Score: {random_search_stage2.best_score_:.4f}\")\n",
        "print(f\"Improvement from Stage 1: {random_search_stage2.best_score_ - random_search_stage1.best_score_:.4f}\")\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in best_params_stage2.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Check for overfitting in Stage 2\n",
        "stage2_train_score = random_search_stage2.best_estimator_.score(X_train_scaled, y_train)\n",
        "stage2_val_score = random_search_stage2.best_estimator_.score(X_val_scaled, y_val)\n",
        "print(f\"\\nStage 2 Overfitting Check:\")\n",
        "print(f\"  Train accuracy: {stage2_train_score:.4f}\")\n",
        "print(f\"  Validation accuracy: {stage2_val_score:.4f}\")\n",
        "print(f\"  Gap: {stage2_train_score - stage2_val_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TogaPjvC_Jat",
        "outputId": "01062861-d6a3-4168-bb82-1f6d79442b8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 2 COMPLETE - Time: 87.1 minutes\n",
            "================================================================================\n",
            "Best CV Score: 0.7668\n",
            "Improvement from Stage 1: 0.0090\n",
            "\n",
            "Best Parameters:\n",
            "  subsample_freq: 1\n",
            "  subsample: 0.95\n",
            "  reg_lambda: 1.0\n",
            "  reg_alpha: 0.1\n",
            "  num_leaves: 31\n",
            "  n_estimators: 500\n",
            "  min_split_gain: 0.01\n",
            "  min_child_weight: 0.01\n",
            "  min_child_samples: 10\n",
            "  max_depth: 7\n",
            "  max_bin: 511\n",
            "  learning_rate: 0.015\n",
            "  colsample_bytree: 0.5\n",
            "\n",
            "Stage 2 Overfitting Check:\n",
            "  Train accuracy: 1.0000\n",
            "  Validation accuracy: 0.7755\n",
            "  Gap: 0.2245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMWithEarlyStopping was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMWithEarlyStopping was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STAGE 3: FINE-TUNING (5,000 iterations)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 3: FINE-TUNING WITH 10-FOLD CV\")\n",
        "print(\"Iterations: 5,000 | CV Folds: 10 | Total Fits: 50,000\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "param_distributions_stage3 = {\n",
        "    'learning_rate': get_range(best_params_stage2['learning_rate'], learning_rate_opts, 2),\n",
        "    'max_depth': get_range(best_params_stage2['max_depth'], max_depth_opts, 1),\n",
        "    'num_leaves': get_range(best_params_stage2['num_leaves'], num_leaves_opts, 2),\n",
        "    'min_child_samples': get_range(best_params_stage2['min_child_samples'], min_child_samples_opts, 2),\n",
        "    'min_child_weight': get_range(best_params_stage2['min_child_weight'], min_child_weight_opts, 1),\n",
        "    'reg_alpha': get_range(best_params_stage2['reg_alpha'], reg_alpha_opts, 2),\n",
        "    'reg_lambda': get_range(best_params_stage2['reg_lambda'], reg_lambda_opts, 2),\n",
        "    'subsample': get_range(best_params_stage2['subsample'], subsample_opts, 2),\n",
        "    'colsample_bytree': get_range(best_params_stage2['colsample_bytree'], colsample_bytree_opts, 2),\n",
        "    'subsample_freq': [best_params_stage2['subsample_freq']],\n",
        "    'min_split_gain': get_range(best_params_stage2['min_split_gain'], min_split_gain_opts, 1),\n",
        "    'max_bin': [best_params_stage2['max_bin']],\n",
        "    'n_estimators': [best_params_stage2['n_estimators']]\n",
        "}\n",
        "\n",
        "print(f\"Stage 3 Search Space:\")\n",
        "for param, values in param_distributions_stage3.items():\n",
        "    print(f\"  {param}: {values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl0N2CCi_LRA",
        "outputId": "b9105466-f895-49ea-d668-2e1e714f976b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 3: FINE-TUNING WITH 10-FOLD CV\n",
            "Iterations: 5,000 | CV Folds: 10 | Total Fits: 50,000\n",
            "================================================================================\n",
            "\n",
            "Stage 3 Search Space:\n",
            "  learning_rate: [0.005, 0.01, 0.015, 0.02, 0.03]\n",
            "  max_depth: [6, 7, 8]\n",
            "  num_leaves: [20, 25, 31, 40, 50]\n",
            "  min_child_samples: [5, 10, 15, 20]\n",
            "  min_child_weight: [0.001, 0.01, 0.1]\n",
            "  reg_alpha: [0.01, 0.05, 0.1, 0.5, 1.0]\n",
            "  reg_lambda: [0.1, 0.5, 1.0, 2.0, 5.0]\n",
            "  subsample: [0.85, 0.9, 0.95, 1.0]\n",
            "  colsample_bytree: [0.5, 0.6, 0.7]\n",
            "  subsample_freq: [1]\n",
            "  min_split_gain: [0.001, 0.01, 0.05]\n",
            "  max_bin: [511]\n",
            "  n_estimators: [500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_stage3 = LGBMWithEarlyStopping(\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lgbm_stage3.set_validation_data(X_val_scaled, y_val)\n",
        "\n",
        "random_search_stage3 = RandomizedSearchCV(\n",
        "    estimator=lgbm_stage3,\n",
        "    param_distributions=param_distributions_stage3,\n",
        "    n_iter=5000,\n",
        "    cv=10,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=44,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "stage3_start = time.time()\n",
        "random_search_stage3.fit(X_train_scaled, y_train)\n",
        "stage3_time = time.time() - stage3_start\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"STAGE 3 COMPLETE - Time: {stage3_time/60:.1f} minutes\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Best CV Score: {random_search_stage3.best_score_:.4f}\")\n",
        "print(f\"Improvement from Stage 2: {random_search_stage3.best_score_ - random_search_stage2.best_score_:.4f}\")\n",
        "print(f\"\\nFinal Best Parameters:\")\n",
        "for param, value in random_search_stage3.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqqQvdYj_Pn5",
        "outputId": "19baf733-07b4-4ce9-b1c4-60c8698e7940"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 5000 candidates, totalling 50000 fits\n",
            "\n",
            "================================================================================\n",
            "STAGE 3 COMPLETE - Time: 438.5 minutes\n",
            "================================================================================\n",
            "Best CV Score: 0.7534\n",
            "Improvement from Stage 2: -0.0134\n",
            "\n",
            "Final Best Parameters:\n",
            "  subsample_freq: 1\n",
            "  subsample: 0.9\n",
            "  reg_lambda: 5.0\n",
            "  reg_alpha: 0.1\n",
            "  num_leaves: 31\n",
            "  n_estimators: 500\n",
            "  min_split_gain: 0.001\n",
            "  min_child_weight: 0.001\n",
            "  min_child_samples: 15\n",
            "  max_depth: 7\n",
            "  max_bin: 511\n",
            "  learning_rate: 0.03\n",
            "  colsample_bytree: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL MODEL WITH EARLY STOPPING VISUALIZATION\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING FINAL MODEL WITH EARLY STOPPING\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create final model with best parameters\n",
        "best_params = random_search_stage3.best_params_.copy()\n",
        "\n",
        "final_model = lgb.LGBMClassifier(\n",
        "    **best_params,\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train with early stopping and monitoring\n",
        "final_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
        "    eval_names=['train', 'valid'],\n",
        "    eval_metric='logloss',\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(period=100)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"\\nBest iteration: {final_model.best_iteration_}\")\n",
        "print(f\"Used {final_model.best_iteration_} out of {best_params['n_estimators']} estimators\")\n"
      ],
      "metadata": {
        "id": "HrTn9pnf_ShM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL EVALUATION\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = final_model.predict(X_train_scaled)\n",
        "y_val_pred = final_model.predict(X_val_scaled)\n",
        "y_test_pred = final_model.predict(X_test_scaled)\n",
        "\n",
        "# Accuracies\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Performance Across All Stages:\")\n",
        "print(f\"  Stage 1 CV Score: {random_search_stage1.best_score_:.4f}\")\n",
        "print(f\"  Stage 2 CV Score: {random_search_stage2.best_score_:.4f}\")\n",
        "print(f\"  Stage 3 CV Score: {random_search_stage3.best_score_:.4f}\")\n",
        "print(f\"  Total Improvement: {random_search_stage3.best_score_ - random_search_stage1.best_score_:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal Model Performance:\")\n",
        "print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nOverfitting Analysis:\")\n",
        "print(f\"  Train-Val Gap: {train_accuracy - val_accuracy:.4f}\")\n",
        "print(f\"  Train-Test Gap: {train_accuracy - test_accuracy:.4f}\")\n",
        "print(f\"  Val-Test Gap: {val_accuracy - test_accuracy:.4f}\")\n",
        "\n",
        "if train_accuracy - test_accuracy > 0.10:\n",
        "    print(\"\\n⚠️  SEVERE WARNING: Significant overfitting detected!\")\n",
        "    print(\"   → Consider: Increase reg_alpha/reg_lambda, reduce max_depth/num_leaves\")\n",
        "elif train_accuracy - test_accuracy > 0.05:\n",
        "    print(\"\\n⚠️  WARNING: Moderate overfitting (train-test gap > 5%)\")\n",
        "    print(\"   → Consider: Slightly increase regularization\")\n",
        "else:\n",
        "    print(\"\\n✓ Model appears to generalize well!\")"
      ],
      "metadata": {
        "id": "ZA3-NEsQ_VJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Classification Report (Test Set):\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "hWX-Dszs_ZXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Confusion Matrix (Test Set):\")\n",
        "print(\"=\"*80)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvsKBGLl_b1f",
        "outputId": "88efa92d-15c6-49e2-94c7-6b2d20b0155a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Confusion Matrix (Test Set):\n",
            "================================================================================\n",
            "[[19  5]\n",
            " [ 6 18]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation for stability\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Cross-Validation Stability Check (10-fold):\")\n",
        "print(\"=\"*80)\n",
        "cv_scores = cross_val_score(final_model, X_train_scaled, y_train, cv=10, scoring='accuracy')\n",
        "print(f\"CV Scores: {cv_scores}\")\n",
        "print(f\"Mean CV Score: {cv_scores.mean():.4f}\")\n",
        "print(f\"Std CV Score: {cv_scores.std():.4f}\")\n",
        "\n",
        "if cv_scores.std() > 0.05:\n",
        "    print(\"\\n⚠️  WARNING: High CV variance - model may be unstable\")\n",
        "else:\n",
        "    print(\"\\n✓ Model shows stable performance across folds\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': features,\n",
        "    'importance': final_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Top 15 Most Important Features:\")\n",
        "print(\"=\"*80)\n",
        "print(feature_importance.head(15).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mBjX_p3_dZA",
        "outputId": "c24ac613-0eaf-4b03-9783-821360c4df38"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Cross-Validation Stability Check (10-fold):\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV Scores: [0.60869565 0.82608696 0.82608696 0.81818182 0.63636364 0.81818182\n",
            " 0.68181818 0.81818182 0.81818182 0.68181818]\n",
            "Mean CV Score: 0.7534\n",
            "Std CV Score: 0.0850\n",
            "\n",
            "⚠️  WARNING: High CV variance - model may be unstable\n",
            "\n",
            "================================================================================\n",
            "Top 15 Most Important Features:\n",
            "================================================================================\n",
            "                    feature  importance\n",
            "                   5man_bpm          74\n",
            "         def_rim_efficiency          69\n",
            "       lineup_depth_quality          56\n",
            "           size_speed_index          54\n",
            "defensive_versatility_score          52\n",
            "      def_experience_impact          50\n",
            "                  5man_dbpm          45\n",
            " def_four_factors_composite          44\n",
            "   def_lineup_depth_quality          39\n",
            "                  3man_obpm          37\n",
            "                  blked_pct          36\n",
            "                 torvik_rtg          36\n",
            "     four_factors_composite          32\n",
            "                   efgd_pct          31\n",
            "                    3pd_pct          30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATIONS\n",
        "# =============================================================================\n",
        "\n",
        "# 1. Training History - Early Stopping Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "lgb.plot_metric(final_model, metric='logloss', ax=axes[0])\n",
        "axes[0].set_title('Training History - Early Stopping', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Iteration', fontsize=12)\n",
        "axes[0].set_ylabel('Log Loss', fontsize=12)\n",
        "axes[0].axvline(x=final_model.best_iteration_, color='red', linestyle='--', label='Best Iteration')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "zjnAq56X_fi9",
        "outputId": "81af6381-948a-446b-8abc-ce49e9743f62"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'No given metric in eval results.'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1090312970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training History - Early Stopping'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/plotting.py\u001b[0m in \u001b[0;36mplot_metric\u001b[0;34m(booster, metric, dataset_names, ax, xlim, ylim, title, xlabel, ylabel, figsize, dpi, grid)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics_for_one\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No given metric in eval results.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_for_one\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'No given metric in eval results.'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Feature Importance\n",
        "top_features = feature_importance.head(20)\n",
        "axes[1].barh(range(len(top_features)), top_features['importance'])\n",
        "axes[1].set_yticks(range(len(top_features)))\n",
        "axes[1].set_yticklabels(top_features['feature'])\n",
        "axes[1].set_xlabel('Importance (Gain)', fontsize=12)\n",
        "axes[1].set_title('Top 20 Feature Importances', fontsize=14, fontweight='bold')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A8D6ngfv_hlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=True,\n",
        "            xticklabels=['Loss', 'Win'], yticklabels=['Loss', 'Win'])\n",
        "plt.title('Confusion Matrix - Test Set (LightGBM)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AYaWmG25_jKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Learning Curves\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Generating Learning Curves...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    final_model, X_train_scaled, y_train,\n",
        "    cv=10, n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    scoring='accuracy',\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(train_sizes, train_mean, label='Training score', color='darkgreen', marker='o', linewidth=2, markersize=8)\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='darkgreen')\n",
        "plt.plot(train_sizes, val_mean, label='Cross-validation score', color='orange', marker='s', linewidth=2, markersize=8)\n",
        "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.15, color='orange')\n",
        "plt.xlabel('Training Set Size', fontsize=12)\n",
        "plt.ylabel('Accuracy Score', fontsize=12)\n",
        "plt.title('Learning Curves - LightGBM with Overfitting Prevention', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OLm4oq2d_kz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning curve analysis\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Learning Curve Analysis:\")\n",
        "print(\"=\"*80)\n",
        "final_train_mean = train_mean[-1]\n",
        "final_val_mean = val_mean[-1]\n",
        "gap = final_train_mean - final_val_mean\n",
        "\n",
        "print(f\"Final training score: {final_train_mean:.4f}\")\n",
        "print(f\"Final validation score: {final_val_mean:.4f}\")\n",
        "print(f\"Gap: {gap:.4f}\")\n",
        "\n",
        "if gap > 0.1:\n",
        "    print(\"\\n⚠️  High variance/overfitting\")\n",
        "    print(\"   → Increase reg_alpha/reg_lambda, reduce max_depth\")\n",
        "elif final_val_mean < 0.65:\n",
        "    print(\"\\n⚠️  High bias/underfitting\")\n",
        "    print(\"   → Reduce regularization, increase max_depth\")\n",
        "else:\n",
        "    print(\"\\n✓ Good bias-variance tradeoff!\")"
      ],
      "metadata": {
        "id": "d-UkLHGs_nUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Stage Comparison\n",
        "stage_scores = [\n",
        "    random_search_stage1.best_score_,\n",
        "    random_search_stage2.best_score_,\n",
        "    random_search_stage3.best_score_\n",
        "]\n",
        "stage_times = [stage1_time/60, stage2_time/60, stage3_time/60]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(['Stage 1', 'Stage 2', 'Stage 3'], stage_scores, marker='o', linewidth=3, markersize=12, color='darkgreen')\n",
        "ax1.set_ylabel('Best CV Score', fontsize=12)\n",
        "ax1.set_title('LightGBM Optimization Progress', fontsize=13, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "for i, score in enumerate(stage_scores):\n",
        "    ax1.text(i, score + 0.002, f'{score:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "ax2.bar(['Stage 1\\n(500 iter)', 'Stage 2\\n(2.5K iter)', 'Stage 3\\n(5K iter)'], stage_times, color=['#2ca02c', '#ff7f0e', '#d62728'])\n",
        "ax2.set_ylabel('Time (minutes)', fontsize=12)\n",
        "ax2.set_title('Time per Stage', fontsize=13, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "for i, t in enumerate(stage_times):\n",
        "    ax2.text(i, t + 1, f'{t:.1f} min', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kN3gijwG_pM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "total_time = stage1_time + stage2_time + stage3_time\n",
        "total_combinations = 500 + 2500 + 5000\n",
        "total_fits = (500 * 5) + (2500 * 5) + (5000 * 10)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL SUMMARY - LightGBM WITH OVERFITTING PREVENTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total combinations tested: {total_combinations:,}\")\n",
        "print(f\"Total model fits: {total_fits:,}\")\n",
        "print(f\"Best iteration used: {final_model.best_iteration_} (early stopped)\")\n",
        "print(f\"\\nTime Breakdown:\")\n",
        "print(f\"  Stage 1: {stage1_time/60:.1f} minutes\")\n",
        "print(f\"  Stage 2: {stage2_time/60:.1f} minutes\")\n",
        "print(f\"  Stage 3: {stage3_time/60:.1f} minutes\")\n",
        "print(f\"  Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
        "print(f\"\\nOptimization Progress:\")\n",
        "print(f\"  Stage 1: {random_search_stage1.best_score_:.4f}\")\n",
        "print(f\"  Stage 2: {random_search_stage2.best_score_:.4f} (+{random_search_stage2.best_score_ - random_search_stage1.best_score_:.4f})\")\n",
        "print(f\"  Stage 3: {random_search_stage3.best_score_:.4f} (+{random_search_stage3.best_score_ - random_search_stage2.best_score_:.4f})\")\n",
        "print(f\"  Total improvement: {random_search_stage3.best_score_ - random_search_stage1.best_score_:.4f}\")\n",
        "print(f\"\\nFinal Performance:\")\n",
        "print(f\"  Train accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"  Validation accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"  Test accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"  Train-test gap: {train_accuracy - test_accuracy:.4f}\")\n",
        "print(f\"  CV stability (std): {cv_scores.std():.4f}\")\n",
        "\n",
        "print(f\"\\nAnti-Overfitting Measures Applied:\")\n",
        "print(f\"  ✓ Early stopping (stopped at {final_model.best_iteration_}/{best_params['n_estimators']})\")\n",
        "print(f\"  ✓ Regularization (L1={best_params['reg_alpha']}, L2={best_params['reg_lambda']})\")\n",
        "print(f\"  ✓ Tree complexity limits (depth={best_params['max_depth']}, leaves={best_params['num_leaves']})\")\n",
        "print(f\"  ✓ Sampling (subsample={best_params['subsample']}, colsample={best_params['colsample_bytree']})\")\n",
        "print(f\"  ✓ Minimum samples (min_child_samples={best_params['min_child_samples']})\")\n",
        "print(f\"  ✓ Train-val-test split for monitoring\")\n",
        "print(f\"  ✓ Cross-validation for stability\")\n",
        "\n",
        "if test_accuracy > 0.75:\n",
        "    print(f\"\\n🏆 Model Performance: OUTSTANDING (>75% accuracy)\")\n",
        "elif test_accuracy > 0.70:\n",
        "    print(f\"\\n🎯 Model Performance: EXCELLENT (>70% accuracy)\")\n",
        "elif test_accuracy > 0.65:\n",
        "    print(f\"\\n✓ Model Performance: GOOD (>65% accuracy)\")\n",
        "elif test_accuracy > 0.60:\n",
        "    print(f\"\\n⚠️  Model Performance: MODERATE (>60% accuracy)\")\n",
        "else:\n",
        "    print(f\"\\n❌ Model Performance: NEEDS IMPROVEMENT (<60% accuracy)\")\n",
        "\n",
        "if train_accuracy - test_accuracy <= 0.05:\n",
        "    print(f\"✓ NO OVERFITTING DETECTED!\")\n",
        "else:\n",
        "    print(f\"⚠️  Some overfitting present - consider increasing regularization\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3-STAGE LightGBM OPTIMIZATION COMPLETE! 🚀\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "33ym97T7_q_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4TNWC5Xf_uxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}